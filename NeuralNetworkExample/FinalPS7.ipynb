{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e81041",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "#os.getcwd()\n",
    "\n",
    "# Testing Function \n",
    "imgDirTrain = r\"C:\\Users\\lexei\\Downloads\\INFO 371\\\\\"\n",
    "imgDirVal = r\"C:\\Users\\lexei\\Downloads\\INFO 371\\\\\"\n",
    "imgWidth, imgHeight = 64, 64\n",
    "imgSize = (imgWidth, imgHeight)\n",
    "## How many color channels\n",
    "imgChannels = 3\n",
    "# Creating Model\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D,\\\n",
    "\tDropout, Flatten, Dense, Activation,\\\n",
    "\tBatchNormalization, Input, Rescaling\n",
    "from keras.utils import image_dataset_from_directory\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import load_img\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# sequential (not recursive) model (one input, one output)\n",
    "# function that llows you to choose how many times you want to run the model to get an average accuracy\n",
    "def modelTimes(imgTrain, imgVal, numTimesRun):\n",
    "    imgDirTrain = imgTrain\n",
    "    imgDirVal = imgVal\n",
    "    accuracies = []\n",
    "    for i in range(numTimesRun) :\n",
    "        model = Sequential([\n",
    "            Input(shape = (imgWidth, imgHeight, imgChannels)),\n",
    "            #Rescaling(1./255),\n",
    "            Conv2D(32, 3, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D(pool_size=2),\n",
    "            Dropout(0.25),\n",
    "\n",
    "            Conv2D(64, 3, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            MaxPooling2D(pool_size=2),\n",
    "            Dropout(0.25),\n",
    "\n",
    "           Conv2D(128, 3, activation='relu'),\n",
    "           BatchNormalization(),\n",
    "           MaxPooling2D(pool_size=2),\n",
    "           Dropout(0.25),\n",
    "\n",
    "            Flatten(),\n",
    "            Dense(256, activation='relu'),\n",
    "            BatchNormalization(),\n",
    "            Dropout(0.5),\n",
    "\n",
    "            Dense(5, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        imgSize = (64, 64)\n",
    "\n",
    "        ## Dataset generators\n",
    "        trainDS = image_dataset_from_directory(\n",
    "            imgDirTrain + \"train/\",\n",
    "            label_mode = \"int\",  # must match with 'sparse_categorical_crossentropy'\n",
    "            color_mode = \"rgb\",  # color channels\n",
    "            batch_size = 20, # WAS 32\n",
    "            image_size = imgSize,\n",
    "            shuffle = True,\n",
    "            seed = 3)\n",
    "\n",
    "        valDS = image_dataset_from_directory(\n",
    "            imgDirVal + \"validation/\",\n",
    "            label_mode = \"int\",  # must match with 'sparse_categorical_crossentropy'\n",
    "            color_mode = \"rgb\",  # color channels\n",
    "            batch_size = 20, # WAS 32\n",
    "            image_size = imgSize,\n",
    "            shuffle = False,\n",
    "            seed = 3)\n",
    "\n",
    "        print(\"Found classes:\", trainDS.class_names)  # alphabetical order\n",
    "        limitedTrainDS = trainDS\n",
    "\n",
    "        yt = np.concatenate([y for x, y in limitedTrainDS], axis=0)\n",
    "        count = pd.Series(yt).value_counts().sort_index()\n",
    "        count.index = trainDS.class_names\n",
    "        print(\"Class distribution in training set:\" + str(count))\n",
    "\n",
    "        print(\"Training\")\n",
    "        history = model.fit(\n",
    "            limitedTrainDS,\n",
    "            epochs = 150  # adjust for more\n",
    "        )\n",
    "\n",
    "        limitedValDS = valDS\n",
    "        predictions = model.predict(limitedValDS)\n",
    "        ## Validation\n",
    "        yv = np.concatenate([y for x, y in limitedValDS], axis=0)\n",
    "        print(\"Predicting\")\n",
    "        phat = model.predict(limitedValDS)\n",
    "        print(\"Predicted array shape:\", phat.shape)\n",
    "        print(\"Example predicted probabilities:\")\n",
    "        print(phat[:5])\n",
    "\n",
    "         ## Convert probabilities to categories:\n",
    "        yhat = np.argmax(phat, axis=-1)\n",
    "\n",
    "        print(f\"Accuracy: {np.mean(yhat == yv):.3}\")\n",
    "        print(confusion_matrix(yhat, yv))\n",
    "        accuracies.append(np.mean(yhat == yv))\n",
    "\n",
    "        def print_labels(dataset, name):\n",
    "            labels = np.concatenate([y.numpy() for x, y in dataset], axis=0)\n",
    "            print(f\"{name} labels: {np.unique(labels)}, counts: {np.bincount(labels)}\")\n",
    "\n",
    "        print_labels(trainDS, \"Train\")\n",
    "        print_labels(valDS, \"Validation\")\n",
    "        print(f\"End of layer {i}\")\n",
    "        print(imgDirVal)\n",
    "\n",
    "        # this is probably not the most convenitent way to do it but I pull from every folder\n",
    "        valDirZN = r\"C:\\Users\\lexei\\Downloads\\INFO 371\\validation\\ZN\\\\\"\n",
    "        valDirEN = r\"C:\\Users\\lexei\\Downloads\\INFO 371\\validation\\EN\\\\\"\n",
    "        valDirRU = r\"C:\\Users\\lexei\\Downloads\\INFO 371\\validation\\RU\\\\\"\n",
    "        valDirTH = r\"C:\\Users\\lexei\\Downloads\\INFO 371\\validation\\TH\\\\\"\n",
    "        valDirDA = r\"C:\\Users\\lexei\\Downloads\\INFO 371\\validation\\DA\\\\\"\n",
    "\n",
    "        # sets up the file paths so that I can randomly pick 20\n",
    "        valDirs = [valDirDA, valDirZN, valDirEN, valDirRU, valDirTH]\n",
    "        allImagePaths = []\n",
    "        for valDir in valDirs:\n",
    "            allImagePaths += [os.path.join(valDir, f) for f in os.listdir(valDir)]\n",
    "\n",
    "        # randomly picks 20 image paths\n",
    "        testFNames = list(np.random.choice(allImagePaths, size=20, replace=False))\n",
    "        print(\"Plot example results\")\n",
    "        labelMap =  {0: \"DA\", 1: \"EN\", 2: \"RU\", 3: \"TH\", 4: \"ZN\"}\n",
    "\n",
    "        plt.figure(figsize=(12, 24))\n",
    "        index = 1\n",
    "        rows, cols = 4, 5\n",
    "        for testFName in testFNames:\n",
    "            img = load_img(testFName, target_size=imgSize)\n",
    "            im = Image.open(testFName).convert(\"RGB\") # have to convert to RGB so the images can go back through the model\n",
    "            im = im.resize(imgSize)\n",
    "            im = np.array(im)\n",
    "            im = np.expand_dims(im, axis=0)\n",
    "            phat = model.predict(im)\n",
    "            yhat = np.argmax(phat, axis=-1)[0]\n",
    "\n",
    "            # picks the photos that the model isn't confident in classifying\n",
    "            if (phat.max() < 0.9): \n",
    "                plt.subplot(rows, cols, index)\n",
    "                plt.imshow(img)\n",
    "                print(\"sample image {} [{}] is {}\".format(testFName, phat, labelMap[yhat]))\n",
    "                plt.xlabel(f\"{labelMap[yhat]} ({phat.max():.3})\")\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            if index >= len(testFNames):\n",
    "                break \n",
    "            index += 1\n",
    "\n",
    "    avgAccuracy = sum(accuracies) / len(accuracies)\n",
    "    print(f\"Average Accuracy: {avgAccuracy}\")\n",
    "    print(\"I am done!\")\n",
    "\n",
    "    return avgAccuracy\n",
    "\n",
    "\n",
    "\n",
    "modelTimes(imgDirTrain, imgDirTrain, 1)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
